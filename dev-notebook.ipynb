{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc71373f",
   "metadata": {},
   "source": [
    "# Minimal working UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b1480",
   "metadata": {},
   "source": [
    "## 0. imports and data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40d9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py  # note: importing h5py multiple times can cause an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc5b631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a4309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these directories have a tiny amount of data for a minimal test\n",
    "# they can be replaced with any other directories depending on OS and preferences\n",
    "\n",
    "train_dir = 'data/h5/train'\n",
    "val_dir = 'data/h5/val'\n",
    "\n",
    "os.listdir(train_dir)\n",
    "train_subjects = os.listdir(train_dir)\n",
    "train_subject = train_subjects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "936effbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_volume(np_arr, chunk_size, stride_size):\n",
    "    \"\"\" Splits an np array of shape (nchannels, nx, ny, nz)\n",
    "    \"\"\"\n",
    "    shape = np_arr.shape\n",
    "    \n",
    "    assert len(shape) == 4\n",
    "    assert shape[1] == shape[2] == shape[3]\n",
    "    \n",
    "    num_chunks = ((shape[1] - chunk_size) // stride_size) + 1\n",
    "\n",
    "    chunks_lvl0 = []\n",
    "    \n",
    "    # triple loop over x, y, z dimensions of volume \n",
    "    for xc in range(num_chunks):\n",
    "        x0 = xc * stride_size\n",
    "        x1 = x0 + chunk_size\n",
    "        \n",
    "        chunks_lvl1 = []\n",
    "        for yc in range(num_chunks):\n",
    "            y0 = yc * stride_size\n",
    "            y1 = y0 + chunk_size\n",
    "            \n",
    "            chunks_lvl2 = []\n",
    "            for zc in range(num_chunks):\n",
    "                z0 = zc * stride_size\n",
    "                z1 = z0 + chunk_size\n",
    "        \n",
    "                chunk = np_arr[:, x0:x1, y0:y1, z0:z1]\n",
    "                chunks_lvl2.append(chunk)\n",
    "            \n",
    "            # zoom out \n",
    "            chunks_lvl1.append(chunks_lvl2)\n",
    "        chunks_lvl0.append(chunks_lvl1)\n",
    "\n",
    "    return chunks_lvl0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2abfa860",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 84  # a little under 256 // 3\n",
    "stride_size = 56  # with a chunk_size of 84, a stride size of 64 allows for some overlap of chunks.\n",
    "\n",
    "raw_chunks = split_volume(raw_np, chunk_size=chunk_size, stride_size=stride_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac8a2411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4, 4, 1, 84, 84, 84)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (x_chunks, y_chunks, z_chunks, channels, xsize, ysize, size)\n",
    "np.array(raw_chunks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f1ee9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datapoint_and_save(h5_dataset, \n",
    "                             chunk_size, \n",
    "                             stride_size, \n",
    "                             save_directory,\n",
    "                             save_filename_prefix):\n",
    "    \n",
    "    \"\"\" Splits an h5 file with compatible-shaped\n",
    "        datasets 'raw' and 'label';\n",
    "        Returns None.\n",
    "        \n",
    "        Its effect is to save out a list of datasets by chunking 'raw' and 'label'\n",
    "        into a smaller new h5 files.\n",
    "        \n",
    "        The newly created h5 files are saved with a filename suffix indicating which\n",
    "        x, y, z chunk of the original data they contain.\n",
    "    \"\"\"    \n",
    "    \n",
    "    raw_np = np.array(h5_dataset.get('raw'))\n",
    "    label_np = np.array(h5_dataset.get('label'))\n",
    "    \n",
    "    split_raw_volumes = split_volume(raw_np, chunk_size, stride_size)\n",
    "    split_label_volumes = split_volume(label_np, chunk_size, stride_size)\n",
    "    \n",
    "    nchunks_1d = len(split_raw_volumes[0])  # assumes x, y, z dimensions are equal\n",
    "    for x in range(nchunks_1d):\n",
    "        for y in range(nchunks_1d):\n",
    "            for z in range(nchunks_1d):\n",
    "                # save out a new h5 file for each chunk of the original volume\n",
    "                print('chunk:', x, y, z)\n",
    "                v1 = split_raw_volumes[x][y][z]\n",
    "                v2 = split_label_volumes[x][y][z]\n",
    "                \n",
    "                file = h5py.File(f'{save_directory}/{save_filename_prefix}_chunk_{x}_{y}_{z}.h5', 'w')\n",
    "                file.create_dataset('raw', data=v1)\n",
    "                file.create_dataset('label', data=v2)\n",
    "                file.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "06a11201",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk: 0 0 0\n",
      "chunk: 0 0 1\n",
      "chunk: 0 0 2\n",
      "chunk: 0 0 3\n",
      "chunk: 0 1 0\n",
      "chunk: 0 1 1\n",
      "chunk: 0 1 2\n",
      "chunk: 0 1 3\n",
      "chunk: 0 2 0\n",
      "chunk: 0 2 1\n",
      "chunk: 0 2 2\n",
      "chunk: 0 2 3\n",
      "chunk: 0 3 0\n",
      "chunk: 0 3 1\n",
      "chunk: 0 3 2\n",
      "chunk: 0 3 3\n",
      "chunk: 1 0 0\n",
      "chunk: 1 0 1\n",
      "chunk: 1 0 2\n",
      "chunk: 1 0 3\n",
      "chunk: 1 1 0\n",
      "chunk: 1 1 1\n",
      "chunk: 1 1 2\n",
      "chunk: 1 1 3\n",
      "chunk: 1 2 0\n",
      "chunk: 1 2 1\n",
      "chunk: 1 2 2\n",
      "chunk: 1 2 3\n",
      "chunk: 1 3 0\n",
      "chunk: 1 3 1\n",
      "chunk: 1 3 2\n",
      "chunk: 1 3 3\n",
      "chunk: 2 0 0\n",
      "chunk: 2 0 1\n",
      "chunk: 2 0 2\n",
      "chunk: 2 0 3\n",
      "chunk: 2 1 0\n",
      "chunk: 2 1 1\n",
      "chunk: 2 1 2\n",
      "chunk: 2 1 3\n",
      "chunk: 2 2 0\n",
      "chunk: 2 2 1\n",
      "chunk: 2 2 2\n",
      "chunk: 2 2 3\n",
      "chunk: 2 3 0\n",
      "chunk: 2 3 1\n",
      "chunk: 2 3 2\n",
      "chunk: 2 3 3\n",
      "chunk: 3 0 0\n",
      "chunk: 3 0 1\n",
      "chunk: 3 0 2\n",
      "chunk: 3 0 3\n",
      "chunk: 3 1 0\n",
      "chunk: 3 1 1\n",
      "chunk: 3 1 2\n",
      "chunk: 3 1 3\n",
      "chunk: 3 2 0\n",
      "chunk: 3 2 1\n",
      "chunk: 3 2 2\n",
      "chunk: 3 2 3\n",
      "chunk: 3 3 0\n",
      "chunk: 3 3 1\n",
      "chunk: 3 3 2\n",
      "chunk: 3 3 3\n"
     ]
    }
   ],
   "source": [
    "# make directory for new chunked dataset\n",
    "# in which each original volume (h5 file)\n",
    "# is saved out as several distinct chunks (h5 files)\n",
    "test_chunk_dir = '.data/test_chunk_dir'  # may need to change for your own tests\n",
    "os.makedirs(test_chunk_dir, exist_ok=True)\n",
    "\n",
    "chunk_size = 84  # a little under 256 // 3\n",
    "stride_size = 56  # with a chunk_size of 84, a stride size of 64 allows for some overlap of chunks.\n",
    "\n",
    "h5_volume = h5py.File(f'{train_dir}/{train_subject}', 'r')\n",
    "h5_prefix = train_subject.split('.')[0] # filter off filename extension .h5\n",
    "\n",
    "split_datapoint_and_save(h5_volume, \n",
    "                         chunk_size, \n",
    "                         stride_size, \n",
    "                         save_directory=test_chunk_dir,\n",
    "                         save_filename_prefix=h5_prefix\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f1aaeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_entire_dataset(data_in_dir, \n",
    "                         data_out_dir, \n",
    "                         chunk_size, \n",
    "                         stride_size\n",
    "                        ):\n",
    "    \"\"\"\n",
    "        Takes in a dataset with one h5 volume's worth of data ('raw' and 'label')\n",
    "        for each subject in 'data_in_dir'\n",
    "        Populates a directory 'data_out_dir' with a several chunk volumes that are\n",
    "        obtained from the original volumes. E.g., 8 octants, each their own h5 file,\n",
    "        for each 1 original h5 file.\n",
    "    \"\"\"\n",
    "    \n",
    "    h5_filenames = sorted(os.listdir(data_in_dir))\n",
    "    for h5_filename in h5_filenames:\n",
    "\n",
    "        chunk_size = 84  # a little under 256 // 3\n",
    "        stride_size = 56  # with a chunk_size of 84, a stride size of 64 allows for some overlap of chunks.\n",
    "\n",
    "        h5_in_file = h5py.File(f'{data_in_dir}/{h5_filename}', 'r')\n",
    "        h5_prefix = h5_filename.split('.')[0] # filter off filename extension .h5\n",
    "\n",
    "        split_datapoint_and_save(h5_in_file, \n",
    "                                 chunk_size, \n",
    "                                 stride_size=stride_size,\n",
    "                                 save_directory=data_out_dir,\n",
    "                                 save_filename_prefix=h5_prefix\n",
    "                                )\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bf943031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_chunked', 'train', '.DS_Store', 'val']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3e6f36b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk: 0 0 0\n",
      "chunk: 0 0 1\n",
      "chunk: 0 0 2\n",
      "chunk: 0 0 3\n",
      "chunk: 0 1 0\n",
      "chunk: 0 1 1\n",
      "chunk: 0 1 2\n",
      "chunk: 0 1 3\n",
      "chunk: 0 2 0\n",
      "chunk: 0 2 1\n",
      "chunk: 0 2 2\n",
      "chunk: 0 2 3\n",
      "chunk: 0 3 0\n",
      "chunk: 0 3 1\n",
      "chunk: 0 3 2\n",
      "chunk: 0 3 3\n",
      "chunk: 1 0 0\n",
      "chunk: 1 0 1\n",
      "chunk: 1 0 2\n",
      "chunk: 1 0 3\n",
      "chunk: 1 1 0\n",
      "chunk: 1 1 1\n",
      "chunk: 1 1 2\n",
      "chunk: 1 1 3\n",
      "chunk: 1 2 0\n",
      "chunk: 1 2 1\n",
      "chunk: 1 2 2\n",
      "chunk: 1 2 3\n",
      "chunk: 1 3 0\n",
      "chunk: 1 3 1\n",
      "chunk: 1 3 2\n",
      "chunk: 1 3 3\n",
      "chunk: 2 0 0\n",
      "chunk: 2 0 1\n",
      "chunk: 2 0 2\n",
      "chunk: 2 0 3\n",
      "chunk: 2 1 0\n",
      "chunk: 2 1 1\n",
      "chunk: 2 1 2\n",
      "chunk: 2 1 3\n",
      "chunk: 2 2 0\n",
      "chunk: 2 2 1\n",
      "chunk: 2 2 2\n",
      "chunk: 2 2 3\n",
      "chunk: 2 3 0\n",
      "chunk: 2 3 1\n",
      "chunk: 2 3 2\n",
      "chunk: 2 3 3\n",
      "chunk: 3 0 0\n",
      "chunk: 3 0 1\n",
      "chunk: 3 0 2\n",
      "chunk: 3 0 3\n",
      "chunk: 3 1 0\n",
      "chunk: 3 1 1\n",
      "chunk: 3 1 2\n",
      "chunk: 3 1 3\n",
      "chunk: 3 2 0\n",
      "chunk: 3 2 1\n",
      "chunk: 3 2 2\n",
      "chunk: 3 2 3\n",
      "chunk: 3 3 0\n",
      "chunk: 3 3 1\n",
      "chunk: 3 3 2\n",
      "chunk: 3 3 3\n",
      "chunk: 0 0 0\n",
      "chunk: 0 0 1\n",
      "chunk: 0 0 2\n",
      "chunk: 0 0 3\n",
      "chunk: 0 1 0\n",
      "chunk: 0 1 1\n",
      "chunk: 0 1 2\n",
      "chunk: 0 1 3\n",
      "chunk: 0 2 0\n",
      "chunk: 0 2 1\n",
      "chunk: 0 2 2\n",
      "chunk: 0 2 3\n",
      "chunk: 0 3 0\n",
      "chunk: 0 3 1\n",
      "chunk: 0 3 2\n",
      "chunk: 0 3 3\n",
      "chunk: 1 0 0\n",
      "chunk: 1 0 1\n",
      "chunk: 1 0 2\n",
      "chunk: 1 0 3\n",
      "chunk: 1 1 0\n",
      "chunk: 1 1 1\n",
      "chunk: 1 1 2\n",
      "chunk: 1 1 3\n",
      "chunk: 1 2 0\n",
      "chunk: 1 2 1\n",
      "chunk: 1 2 2\n",
      "chunk: 1 2 3\n",
      "chunk: 1 3 0\n",
      "chunk: 1 3 1\n",
      "chunk: 1 3 2\n",
      "chunk: 1 3 3\n",
      "chunk: 2 0 0\n",
      "chunk: 2 0 1\n",
      "chunk: 2 0 2\n",
      "chunk: 2 0 3\n",
      "chunk: 2 1 0\n",
      "chunk: 2 1 1\n",
      "chunk: 2 1 2\n",
      "chunk: 2 1 3\n",
      "chunk: 2 2 0\n",
      "chunk: 2 2 1\n",
      "chunk: 2 2 2\n",
      "chunk: 2 2 3\n",
      "chunk: 2 3 0\n",
      "chunk: 2 3 1\n",
      "chunk: 2 3 2\n",
      "chunk: 2 3 3\n",
      "chunk: 3 0 0\n",
      "chunk: 3 0 1\n",
      "chunk: 3 0 2\n",
      "chunk: 3 0 3\n",
      "chunk: 3 1 0\n",
      "chunk: 3 1 1\n",
      "chunk: 3 1 2\n",
      "chunk: 3 1 3\n",
      "chunk: 3 2 0\n",
      "chunk: 3 2 1\n",
      "chunk: 3 2 2\n",
      "chunk: 3 2 3\n",
      "chunk: 3 3 0\n",
      "chunk: 3 3 1\n",
      "chunk: 3 3 2\n",
      "chunk: 3 3 3\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 84\n",
    "stride_size = 56\n",
    "\n",
    "data_in_dir = train_dir\n",
    "data_out_dir = 'data/h5/train_chunked'  # may need to change for your own tests\n",
    "os.makedirs(data_out_dir, exist_ok=True)\n",
    "\n",
    "chunk_entire_dataset(data_in_dir=data_in_dir,\n",
    "                     data_out_dir=data_out_dir, \n",
    "                     chunk_size=chunk_size, \n",
    "                     stride_size=stride_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2708154",
   "metadata": {},
   "source": [
    "## 1. Dataloader logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8a6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "\n",
    "    \"\"\" A custom Dataset class to iterate over subjects.\n",
    "        This Dataset assumes that the data take the following form:\n",
    "            data_dir/\n",
    "                -- subject0.hdf5 (file with two datasets)\n",
    "                    -- x_name: 4D array\n",
    "                    -- y_name: 4D array\n",
    "                -- subject1.hdf5 (next file with two datasets)\n",
    "                    -- ...\n",
    "        Note also that this directory should not contain any other files\n",
    "        besides h5 files for subjects intended to be included in this dataset.\n",
    "        -----\n",
    "        Arguments:\n",
    "            data_dir\n",
    "            x_name\n",
    "            y_name\n",
    "            ordered_subject_list\n",
    "        -----       \n",
    "        Returns:\n",
    "            Pytorch index-based Dataset where each sample is an x, y pair of tensors\n",
    "                corresponding to a 3D T1 scan (padded to 4D with a dummy dimension)\n",
    "                and a 4D set of anatomical labels (one-hot encoded classes)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_dir, \n",
    "                 x_name=None,\n",
    "                 y_name=None,\n",
    "                 ordered_subject_list=None):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        # parse default args\n",
    "        x_name = 'raw' if x_name is None else x_name\n",
    "        y_name = 'label' if y_name is None else y_name\n",
    "        self.x_name = x_name\n",
    "        self.y_name = y_name\n",
    "        \n",
    "        # parse subject ordering, if specified\n",
    "        if ordered_subject_list is None:\n",
    "            ordered_subject_list = sorted(os.listdir(data_dir))\n",
    "        self.subjects = ordered_subject_list\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subjects)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        subject = self.subjects[index]  # Select the current datapoint (subject)    \n",
    "        h5 = h5py.File(f'{self.data_dir}/{subject}', 'r')\n",
    "        \n",
    "        x_np = h5.get(self.x_name)\n",
    "        y_np = h5.get(self.y_name)\n",
    "        \n",
    "        x = t.from_numpy(np.array(x_np))\n",
    "        y = t.from_numpy(np.array(y_np))\n",
    "        \n",
    "        h5.close() # close the h5 file to avoid extra memory usage\n",
    "\n",
    "        # If necessary, apply any preprocessing or transformations to the data\n",
    "        # data = ...\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48ca6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = HDF5Dataset(data_dir=train_dir)\n",
    "ds_val = HDF5Dataset(data_dir=val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1ce405",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a8f47",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe51934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translated from imports of unet3d.model\n",
    "from unet3d.buildingblocks import DoubleConv, ResNetBlock, ResNetBlockSE, \\\n",
    "    create_decoders, create_encoders\n",
    "from unet3d.utils import get_class, number_of_features_per_level\n",
    "from unet3d.model import UNet3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438d7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 102\n",
    "\n",
    "model = UNet3D(in_channels=in_channels, out_channels=out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e75b726b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet3D(\n",
       "  (encoders): ModuleList(\n",
       "    (0): Encoder(\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(1, 1, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Encoder(\n",
       "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Encoder(\n",
       "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): Decoder(\n",
       "      (upsampling): InterpolateUpsampling()\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(768, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (upsampling): InterpolateUpsampling()\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (upsampling): InterpolateUpsampling()\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv3d(64, 102, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (final_activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifying \"float\" may or may not be necessary on the GPU\n",
    "# but it is required on CPU\n",
    "def get_device():\n",
    "    device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "    return device\n",
    "    \n",
    "model.to(device, dtype=float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91a16ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "297eb35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward pass\n",
    "di_train = iter(dl_train)\n",
    "di_val = iter(dl_val)\n",
    "\n",
    "xt, yt = next(di_train)\n",
    "xv, yv = next(di_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2832740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this cell doesn't run, training won't work either\n",
    "xt = xt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02e0963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet3D(\n",
       "  (encoders): ModuleList(\n",
       "    (0): Encoder(\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(1, 1, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 32, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Encoder(\n",
       "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Encoder(\n",
       "      (pooling): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0): Decoder(\n",
       "      (upsampling): InterpolateUpsampling()\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 768, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(768, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (upsampling): InterpolateUpsampling()\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 384, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(384, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 128, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (upsampling): InterpolateUpsampling()\n",
       "      (basic_module): DoubleConv(\n",
       "        (SingleConv1): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 192, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(192, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "        (SingleConv2): SingleConv(\n",
       "          (groupnorm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "          (ReLU): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv3d(64, 102, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  (final_activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc343c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e62d474",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GET was unable to find an engine to execute this computation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/groups/jyeatman/samjohns-projects/neuro3dunet/unet3d/model.py:78\u001b[0m, in \u001b[0;36mAbstractUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m encoders_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders:\n\u001b[0;32m---> 78\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# reverse the encoder outputs to be aligned with the decoder\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     encoders_features\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, x)\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/groups/jyeatman/samjohns-projects/neuro3dunet/unet3d/buildingblocks.py:280\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling(x)\n\u001b[0;32m--> 280\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GET was unable to find an engine to execute this computation"
     ]
    }
   ],
   "source": [
    "output = model(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e3fb4",
   "metadata": {},
   "source": [
    "## 3. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints'  # change this based on your OS and preferences\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "642ea0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Other training parameters\n",
    "epochs = 10\n",
    "lr_scheduler_patience = 3\n",
    "lr_scheduler_factor = 0.1\n",
    "\n",
    "def train(dl_train, \n",
    "          dl_val, \n",
    "          model, \n",
    "          optimizer,\n",
    "          criterion,\n",
    "          n_batches=1e3,\n",
    "          lr_scheduler=None,\n",
    "         ):\n",
    "    \"\"\" Function to wrap the main training loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = get_device()  # defined in above section\n",
    "    \n",
    "    # parse default parameters\n",
    "    if lr_scheduler is None:\n",
    "        default_lr_scheduler_patience = 3\n",
    "        default_lr_scheduler_factor = 0.1\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                            patience=default_lr_scheduler_patience, \n",
    "                                                            factor=default_lr_scheduler_factor\n",
    "                                                           )\n",
    "  \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in dl_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = outputs.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dl_val:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = outputs.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Compute average loss\n",
    "        train_loss /= len(dl_train)\n",
    "        val_loss /= len(dl_val)\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check if current validation loss is the best so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Save the model checkpoint if desired\n",
    "            \n",
    "\n",
    "        # Check early stopping condition if desired\n",
    "        # TODO\n",
    "\n",
    "    # Training complete\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ea54a58",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mixed dtype (CPU): expect input to have scalar type of BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dl_train, dl_val, model, optimizer, criterion, epochs, lr_scheduler)\u001b[0m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/groups/jyeatman/samjohns-projects/neuro3dunet/unet3d/model.py:78\u001b[0m, in \u001b[0;36mAbstractUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m encoders_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders:\n\u001b[0;32m---> 78\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# reverse the encoder outputs to be aligned with the decoder\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     encoders_features\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, x)\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/groups/jyeatman/samjohns-projects/neuro3dunet/unet3d/buildingblocks.py:280\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling(x)\n\u001b[0;32m--> 280\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/modules/normalization.py:273\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/groups/jyeatman/software/anaconda3/envs/pytorch-3dunet/lib/python3.9/site-packages/torch/nn/functional.py:2530\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2529\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mixed dtype (CPU): expect input to have scalar type of BFloat16"
     ]
    }
   ],
   "source": [
    "train(dl_train, dl_val, model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f2530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "678722d6",
   "metadata": {},
   "source": [
    "## 4. Evaluation, visualizaitons, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3722b26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-3dunet]",
   "language": "python",
   "name": "conda-env-pytorch-3dunet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
